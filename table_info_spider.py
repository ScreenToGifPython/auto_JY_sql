# -*- encoding: utf-8 -*-
"""
@File: table_info_spider.py
@Modify Time: 2025/7/5
@Author: KevinChen
@Descriptions: æœ¬è„šæœ¬ç”¨äºé€šè¿‡è®¿é—®èšæºå­—å…¸è¡¨ https://dd.gildata.com/#/login,
è·å–æŒ‡å®šè¡¨çš„è¡¨ä¿¡æ¯, è·å–ç»“æœä¼šä»¥Jsonæ ¼å¼ä¿å­˜åˆ°å½“å‰æ–‡ä»¶å¤¹çš„ table_definitions.json æ–‡ä»¶å†…ã€‚
"""
import json
import traceback
import os
import httpx
import re
import subprocess
import sys
import argparse
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from openai import OpenAI
from bs4 import BeautifulSoup
from bs4.element import Comment
import time
import random


# --- è®°å¿†/ç¼“å­˜/æ•°æ®åº“ åŠŸèƒ½ ---
def load_json_file(file_name):
    file_path = os.path.join(os.getcwd(), file_name)
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}
    return {}


def save_json_file(data, file_name):
    file_path = os.path.join(os.getcwd(), file_name)
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)


def random_sleep(min_sleep_time, max_sleep_time):
    sleep_time = random.uniform(min_sleep_time, max_sleep_time)
    print(f"ğŸ˜´ ä¼‘çœ  {sleep_time:.2f} ç§’...")
    time.sleep(sleep_time)


# --- æµè§ˆå™¨ä¸å…ƒç´ æ“ä½œ ---
def launch_browser(window_size):
    options = webdriver.ChromeOptions()
    if window_size:
        width, height = window_size
        options.add_argument(f"--window-size={width},{height}")
    return webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)


def find_element_safely(driver, by, value, explicit_wait_timeout):
    try:
        wait = WebDriverWait(driver, explicit_wait_timeout)
        by_obj = getattr(By, by.upper())
        return wait.until(EC.presence_of_element_located((by_obj, value)))
    except Exception:
        print(traceback.format_exc())
        return None


# --- æ•°æ®æå–ä¸AIæ•´ç†æ¨¡å— ---
def simplify_html(html_source):
    soup = BeautifulSoup(html_source, 'html.parser')
    # ç§»é™¤è„šæœ¬ã€æ ·å¼ã€é“¾æ¥ã€å…ƒæ•°æ®ã€iframeã€å›¾ç‰‡ã€SVGç­‰éå†…å®¹æ€§æ ‡ç­¾
    for tag in soup(['script', 'style', 'link', 'meta', 'iframe', 'img', 'svg', 'path', 'canvas', 'noscript']):
        tag.decompose()
    # ç§»é™¤æ‰€æœ‰HTMLæ³¨é‡Š
    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
        comment.extract()

    # ç§»é™¤é™¤å…³é”®å±æ€§å¤–çš„æ‰€æœ‰å±æ€§
    for tag in soup.find_all(True):
        if tag.name not in ['html', 'body']:
            attrs_to_keep = ['id', 'class', 'name', 'type', 'placeholder', 'value', 'aria-label', 'role', 'href', 'src']
            all_attrs = list(tag.attrs.keys())
            for attr in all_attrs:
                if attr not in attrs_to_keep:
                    del tag.attrs[attr]

    body = soup.find('body')
    if body:
        return str(body.prettify())
    return ""


def get_locator_from_ai(html_source, element_description, api_key, base_url, model_name):
    print("ğŸ¤– æ­£åœ¨ç²¾ç®€HTMLä»¥é€‚åº”æ¨¡å‹ä¸Šä¸‹æ–‡...")
    simplified_html = simplify_html(html_source)
    if len(simplified_html) > 200000:
        print(f"âŒ ç²¾ç®€åçš„HTMLä»ç„¶è¿‡é•¿ ({len(simplified_html)} chars)ï¼Œè·³è¿‡APIè°ƒç”¨ã€‚")
        return None

    print("ğŸ¤– æ­£å¼è°ƒç”¨å¤§æ¨¡å‹APIè¿›è¡Œåˆ†æ...")
    try:
        http_client = httpx.Client(verify=False)
        client = OpenAI(api_key=api_key, base_url=base_url, http_client=http_client)
        system_prompt = (
            "You are an expert web automation assistant. Your task is to analyze HTML source code "
            "and return a single, precise, and robust Selenium locator for a requested element. "
            "You must return the result as a JSON object with two keys: 'by' and 'value'. "
            "The 'by' key must be one of the following strings: 'ID', 'NAME', 'CLASS_NAME', 'TAG_NAME', 'LINK_TEXT', 'PARTIAL_LINK_TEXT', 'CSS_SELECTOR', 'XPATH'. "
            "The 'value' key is the corresponding locator string."
        )
        user_prompt = (
            f"Based on the following HTML, find the locator for the '{element_description}'.\n\n"
            f"HTML:\n```html\n{simplified_html}\n```\n\n"
            "Return only the JSON object."
        )
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            stream=False
        )
        response_text = response.choices[0].message.content
        print(f"ğŸ¤– å¤§æ¨¡å‹è¿”å›åŸå§‹ç»“æœ: {response_text}")
        json_part = response_text[response_text.find('{'):response_text.rfind('}') + 1]
        locator = json.loads(json_part)
        if isinstance(locator, dict) and 'by' in locator and 'value' in locator:
            return locator
        else:
            print("âŒ å¤§æ¨¡å‹è¿”å›çš„JSONæ ¼å¼ä¸æ­£ç¡®ã€‚")
            return None
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(traceback.format_exc())
        return None


def scrape_table_details(driver):
    """ä»è¡¨è¯¦æƒ…é¡µHTMLä¸­æŠ“å–æ‰€æœ‰ä¿¡æ¯(v14.4 ç»ˆæä¿®æ­£æå–ä¸æ ¼å¼)"""
    print("ğŸ” æ­£åœ¨ä½¿ç”¨Seleniumç›´æ¥æå–é¡µé¢å…ƒç´ ...")
    scraped_data = {"basic_info": {}, "columns_data": [], "notes_map": {}}

    # --- ç»ˆæä¿®æ­£ 1: ä½¿ç”¨æ›´ç²¾ç¡®çš„XPathæå–åŸºæœ¬ä¿¡æ¯ ---
    # å°è¯•æå–â€œè¡¨ä¸­æ–‡åâ€åŠå…¶ä»–åŸºæœ¬ä¿¡æ¯
    try:
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        # æŸ¥æ‰¾åŒ…å«ä¸­æ–‡è¡¨åçš„spanæ ‡ç­¾
        chinese_name_span = soup.find('span', {'ng-bind-html': re.compile(r'table\.tableChiName')})
        if chinese_name_span:
            scraped_data["basic_info"]["tableChiName"] = chinese_name_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'è¡¨ä¸­æ–‡å'ã€‚")

        # æå– description
        description_span = soup.find('span', {'ng-bind-html': re.compile(r'table\.description')})
        if description_span:
            scraped_data["basic_info"]["description"] = description_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'description'ã€‚")

        # æå– tableUpdateTime
        table_update_time_span = soup.find('span', {'ng-bind': 'table.tableUpdateTime'})
        if table_update_time_span:
            scraped_data["basic_info"]["tableUpdateTime"] = table_update_time_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'tableUpdateTime'ã€‚")

        # æå– key (ä¸šåŠ¡å”¯ä¸€æ€§)
        key_span = soup.find('span', {'ng-bind': "index.columnName || 'æ— '"})
        if key_span:
            scraped_data["basic_info"]["key"] = key_span.get_text(strip=True)
        else:
            print("âš ï¸ æœªèƒ½æå–åˆ°'key'ã€‚")

    except Exception as e:
        print(traceback.format_exc())
        print(f"âš ï¸ æå–åŸºæœ¬ä¿¡æ¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    # 2. æå–åˆ—ä¿¡æ¯ (é€»è¾‘ä¸å˜)
    try:
        column_table_element = driver.find_element(By.CSS_SELECTOR, 'table.table-column.table-interval-bg')
        column_table_html = driver.execute_script("return arguments[0].outerHTML;", column_table_element)
        soup_column_table = BeautifulSoup(column_table_html, 'html.parser')

        headers = [th.text.strip() for th in soup_column_table.find('thead').find_all('th')]
        # ä¿®æ­£ï¼šng-repeatå¯èƒ½åœ¨tbodyä¸Šï¼Œè€Œä¸æ˜¯trä¸Šï¼ŒæŸ¥æ‰¾æ‰€æœ‰træ›´ç¨³å¦¥
        rows = soup_column_table.find('tbody').find_all('tr')

        for row in rows:
            cells = row.find_all('td')
            if not cells:
                continue  # è·³è¿‡ç©ºè¡Œæˆ–è¡¨å¤´å†…çš„è¡Œ
            col_dict = {}
            for i, header in enumerate(headers):
                if i < len(cells):
                    col_dict[header] = cells[i].text.strip()
            if col_dict:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—å…¸
                scraped_data["columns_data"].append(col_dict)
    except Exception as e:
        print(traceback.format_exc())
        print(f"âš ï¸ æœªèƒ½ä½¿ç”¨ Selenium å®šä½åˆ°åˆ—è¡¨æ ¼æˆ–æå–åˆ—æ•°æ®ã€‚é”™è¯¯: {e}")

    # --- æ ¸å¿ƒä¿®æ­£ 2: ä¿®æ­£å¤‡æ³¨ä¿¡æ¯æå– ---
    try:
        remark_table_element = driver.find_element(By.CSS_SELECTOR, 'table.table-remark')
        rows = remark_table_element.find_elements(By.TAG_NAME, 'tr')

        for row in rows:
            cells = row.find_elements(By.TAG_NAME, 'td')
            if len(cells) == 2:
                key_element = cells[0]
                value_element = cells[1]  # This is the WebElement for the remark content

                key = key_element.text.strip().replace('[', '').replace(']', '').strip()
                initial_value = value_element.text.strip()

                # Check for "æ›´å¤š" button and click if present
                more_button = None
                try:
                    # Look for a span or a tag with text 'æ›´å¤š' within the value_element
                    more_button = value_element.find_element(By.XPATH, ".//span[text()='æ›´å¤š']")
                except:
                    try:
                        more_button = value_element.find_element(By.XPATH, ".//a[text()='æ›´å¤š']")
                    except:
                        pass  # No 'æ›´å¤š' button found

                if more_button:
                    print(f"â„¹ï¸ å‘ç°å¤‡æ³¨ '{key}' å­˜åœ¨ 'æ›´å¤š' æŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»å±•å¼€...")
                    driver.execute_script("arguments[0].click();", more_button)  # Use JS click for robustness
                    time.sleep(1)  # Give time for content to expand
                    # Re-get the text after expansion
                    value = value_element.text.strip()
                    print(f"âœ… å¤‡æ³¨ '{key}' å·²å±•å¼€ã€‚")
                else:
                    value = initial_value  # No 'æ›´å¤š' button, use initial value

                if key:  # ç¡®ä¿keyä¸ä¸ºç©º
                    scraped_data["notes_map"][key] = value
    except Exception as e:
        print(traceback.format_exc())
        print(f"âš ï¸ æœªèƒ½ä½¿ç”¨ Selenium å®šä½åˆ°å¤‡æ³¨è¡¨æ ¼æˆ–æå–å¤‡æ³¨æ•°æ®ã€‚é”™è¯¯: {e}")

    print(f"âœ… æŠ“å–å®Œæˆï¼šæ‰¾åˆ° {len(scraped_data['columns_data'])} åˆ—æ•°æ®ï¼Œ{len(scraped_data['notes_map'])} æ¡å¤‡æ³¨ã€‚")
    return scraped_data


def simplify_comment_with_llm(comment_text, api_key, base_url, model_name):
    print(comment_text)
    """
    ä½¿ç”¨å¤§æ¨¡å‹ç®€åŒ–å•ä¸ªå¤‡æ³¨ä¿¡æ¯ï¼Œæ ¹æ®å†…å®¹é€‰æ‹©ä¸åŒçš„æç¤ºè¯ï¼Œå¹¶ä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å€¼æ˜ å°„ã€‚
    """
    if not comment_text or comment_text.strip() == "":
        return ""

    print(f"ğŸ¤– æ­£åœ¨å¤„ç†å¤‡æ³¨: '{comment_text[:50]}...' ")

    # ä¼˜å…ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–â€œæ•°å­—-æè¿°â€åˆ—è¡¨
    if "CT_SystemConst" in comment_text and "DMå­—æ®µ" in comment_text:
        value_pairs = re.findall(r'(\d+)[-â€”]([^\s,ï¼Œã€‚ï¼›ï¼›\n\r]+)', comment_text)
        if value_pairs:
            formatted = [f"{code}-{desc}" for code, desc in value_pairs]
            extracted_values = ", ".join(formatted)
            print(f"âœ… æ­£åˆ™è¡¨è¾¾å¼æå–åˆ°å€¼æ˜ å°„: '{extracted_values[:50]}...' ")
            return extracted_values

    # Fallback to LLM if regex doesn't apply or doesn't find anything
    try:
        http_client = httpx.Client(verify=False)
        client = OpenAI(api_key=api_key, base_url=base_url, http_client=http_client)

        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åº“æ–‡æ¡£åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ç®€åŒ–æ•°æ®åº“å­—æ®µçš„å¤‡æ³¨ä¿¡æ¯ã€‚"
            "ä½ ä¼šæ”¶åˆ°ä¸€ä¸ªå¤‡æ³¨æ–‡æœ¬ã€‚"
            "è¯·ç”¨æœ€ç®€æ´çš„è¯­è¨€æ€»ç»“å…¶æ ¸å¿ƒå«ä¹‰ã€ä¸å…¶ä»–è¡¨çš„å…³è”æˆ–å…³é”®ä¸šåŠ¡é€»è¾‘ï¼Œå»é™¤å†—ä½™çš„è§£é‡Šæ€§æ–‡å­—ã€‚"
            "å¦‚æœå¤‡æ³¨å·²ç»éå¸¸ç®€æ´ï¼Œè¯·ç›´æ¥è¿”å›åŸå§‹å¤‡æ³¨ã€‚"
            "åªè¿”å›ç®€åŒ–åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜ã€‚"
        )
        user_prompt = f"è¯·ç®€åŒ–ä»¥ä¸‹å¤‡æ³¨ï¼š\n\n{comment_text}"

        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,  # è¾ƒä½çš„æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„ç»“æœ
            stream=False
        )
        simplified_text = response.choices[0].message.content.strip()
        print(f"âœ… å¤§æ¨¡å‹ç®€åŒ–å®Œæˆ: '{simplified_text[:50]}...' ")
        return simplified_text
    except Exception as e:
        print(f"âŒ è°ƒç”¨å¤§æ¨¡å‹ç®€åŒ–å¤‡æ³¨æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        print(traceback.format_exc())
        return comment_text  # å¤±è´¥æ—¶è¿”å›åŸå§‹å¤‡æ³¨


def organize_data_locally(table_name, scraped_data, api_key, base_url, model_name):
    """
    ä½¿ç”¨çº¯Pythonä»£ç å°†æŠ“å–çš„æ•°æ®æ•´ç†æˆæœ€ç»ˆçš„JSONç»“æ„ï¼Œå¹¶å¯¹æ¯ä¸ªå¤‡æ³¨è°ƒç”¨å¤§æ¨¡å‹è¿›è¡Œç®€åŒ–ã€‚
    """
    print("ğŸ æ­£åœ¨ä½¿ç”¨æœ¬åœ°ä»£ç è¿›è¡Œæ•°æ®æ•´åˆä¸æ¸…æ´—...")

    processed_columns = []
    notes_map = scraped_data.get("notes_map", {})

    for col in scraped_data.get("columns_data", []):
        processed_col = col.copy()  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        remark_key = processed_col.get("å¤‡æ³¨")
        if remark_key and remark_key in notes_map:
            original_remark = notes_map[remark_key]
            simplified_remark = simplify_comment_with_llm(original_remark, api_key, base_url, model_name)
            processed_col["å¤‡æ³¨"] = simplified_remark
        processed_columns.append(processed_col)

    # æ„å»ºæœ€ç»ˆçš„JSONå¯¹è±¡
    final_table_definition = {
        "tableName": table_name,
        "tableChiName": scraped_data.get("basic_info", {}).get("tableChiName", ""),
        "description": scraped_data.get("basic_info", {}).get("description", ""),
        "tableUpdateTime": scraped_data.get("basic_info", {}).get("tableUpdateTime", ""),
        "key": scraped_data.get("basic_info", {}).get("key", ""),
        "columns": processed_columns
    }

    print("âœ… æœ¬åœ°æ•°æ®æ•´ç†å®Œæˆã€‚")
    return final_table_definition


# --- æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ ---
def process_table_details_page(driver, table_name, explicit_wait_timeout, output_json_file, api_key, base_url,
                               model_name, min_sleep_time, max_sleep_time):
    """å¤„ç†è¡¨è¯¦æƒ…é¡µï¼Œæå–ã€æ•´ç†å¹¶ä¿å­˜æ•°æ®"""
    print("--- å¯¼èˆªåˆ°è¯¦æƒ…é¡µæˆåŠŸï¼Œå¼€å§‹æ•°æ®æå–æµç¨‹ --- ")
    scraped_data = {"columns_data": []}  # åˆå§‹åŒ–ä¸ºç©ºï¼Œç¡®ä¿å³ä½¿å¤±è´¥ä¹Ÿæœ‰æ­¤é”®
    table_row_locator = (By.CSS_SELECTOR, "table.table-column.table-interval-bg tbody tr")

    for attempt in range(3):  # æœ€å¤šå°è¯•3æ¬¡
        try:
            wait = WebDriverWait(driver, explicit_wait_timeout)
            print(f"â³ å°è¯• {attempt + 1}/3: æ­£åœ¨ç­‰å¾…å…ƒç´ å‡ºç°: {table_row_locator}")
            wait.until(EC.presence_of_element_located(table_row_locator))
            random_sleep(min_sleep_time, max_sleep_time)
            print("--- åˆ—è¡¨æ ¼å†…å®¹å·²åŠ è½½ï¼Œå¼€å§‹æŠ“å–æ•°æ® --- ")

            scraped_data = scrape_table_details(driver)
            if scraped_data and len(scraped_data.get("columns_data", [])) >= 2:
                print(f"âœ… å°è¯• {attempt + 1}/3: æˆåŠŸæŠ“å–åˆ° {len(scraped_data['columns_data'])} æ¡åˆ—ä¿¡æ¯ã€‚")
                break  # æˆåŠŸæŠ“å–åˆ°è¶³å¤Ÿæ•°æ®ï¼Œè·³å‡ºå¾ªç¯
            else:
                print(
                    f"âš ï¸ å°è¯• {attempt + 1}/3: æŠ“å–åˆ°çš„åˆ—ä¿¡æ¯ä¸è¶³2æ¡ ({len(scraped_data.get('columns_data', []))} æ¡)ã€‚")
                if attempt < 2:  # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œåˆ™åˆ·æ–°é¡µé¢é‡è¯•
                    print("ğŸ”„ åˆ·æ–°é¡µé¢å¹¶é‡è¯•...")
                    driver.refresh()
                    random_sleep(min_sleep_time, max_sleep_time)  # åˆ·æ–°åç­‰å¾…é¡µé¢é‡æ–°åŠ è½½
                else:
                    print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œå°†ä½¿ç”¨å½“å‰æŠ“å–åˆ°çš„æ•°æ®ã€‚")
        except Exception as e:
            print(f"âŒ å°è¯• {attempt + 1}/3: åœ¨ç­‰å¾…æˆ–æŠ“å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print(traceback.format_exc())
            if attempt < 2:
                print("ğŸ”„ åˆ·æ–°é¡µé¢å¹¶é‡è¯•...")
                driver.refresh()
                random_sleep(min_sleep_time, max_sleep_time)
            else:
                print("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ— æ³•ç»§ç»­ã€‚")
                # è°ƒè¯•æ­¥éª¤ï¼šä¿å­˜å½“å‰é¡µé¢HTMLï¼Œä»¥ä¾¿åˆ†æ
                debug_file = "debug_page_source.html"
                with open(debug_file, "w", encoding="utf-8") as f:
                    f.write(driver.page_source)
                print(f"â„¹ï¸ ä¸ºäº†ä¾¿äºè°ƒè¯•ï¼Œå½“å‰çš„é¡µé¢HTMLå·²ä¿å­˜åˆ°: {os.path.abspath(debug_file)}")
                return  # å½»åº•å¤±è´¥ï¼Œè¿”å›

    if not scraped_data or not scraped_data.get("columns_data"):
        print("âŒ æœ€ç»ˆæœªèƒ½ä»é¡µé¢æŠ“å–åˆ°ä»»ä½•åˆ—ä¿¡æ¯ï¼Œæ— æ³•ç»§ç»­ã€‚")
        return

    final_table_json = organize_data_locally(table_name, scraped_data, api_key, base_url, model_name)
    if final_table_json:
        print("ğŸ’¾ æ­£åœ¨ä»¥å¢é‡/æ›´æ–°æ¨¡å¼ä¿å­˜æ•°æ®...")
        database = load_json_file(output_json_file)
        database[table_name] = final_table_json
        save_json_file(database, output_json_file)
        print(f"âœ… æˆåŠŸå°†è¡¨ '{table_name}' çš„å®šä¹‰ä¿å­˜åˆ° {output_json_file}ã€‚")
    else:
        print("âŒ AIæœªèƒ½æˆåŠŸæ•´ç†æ•°æ®ï¼Œæœ¬æ¬¡ä¸ä¿å­˜ã€‚")


def login_and_search(driver, login_wait_time, login_url, explicit_wait_timeout, table_to_search, api_key, base_url,
                     model_name, min_sleep_time, max_sleep_time):
    element_cache_file = "element_cache.json"
    output_json_file = "table_definitions.json"
    driver.get(login_url)
    cache = load_json_file(element_cache_file)

    print("è¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­æ‰‹åŠ¨å®Œæˆç™»å½•...")
    print("è„šæœ¬æ­£åœ¨è‡ªåŠ¨æ£€æµ‹ç™»å½•çŠ¶æ€...")

    wait_start_time = time.time()
    while True:
        current_url = driver.current_url
        elapsed_time = time.time() - wait_start_time

        if "/login" not in current_url:
            print("\nâœ… ç™»å½•æˆåŠŸï¼å·²è·³è½¬åˆ°ä¸»é¡µï¼Œè„šæœ¬å°†ç»§ç»­æ‰§è¡Œã€‚")
            time.sleep(3)  # ç­‰å¾…é¡µé¢èµ„æºåŠ è½½
            break

        if elapsed_time > login_wait_time:
            print(f"\nâŒ ç™»å½•è¶…æ—¶ï¼ˆè¶…è¿‡ {login_wait_time} ç§’ï¼‰ï¼Œè¯·æ£€æŸ¥ç™»å½•æˆ–ç½‘ç»œçŠ¶æ€ã€‚ç¨‹åºç»ˆæ­¢ã€‚")
            return  # ç›´æ¥é€€å‡ºå‡½æ•°

        print(f"â³ ç­‰å¾…ç™»å½•ä¸­... å·²ç­‰å¾… {int(elapsed_time)} ç§’...", end="")
        random_sleep(min_sleep_time, max_sleep_time)  # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡

    search_box_locator = cache.get("search_box")
    search_box = None
    if search_box_locator:
        print("ğŸ§  æ­£åœ¨å°è¯•ä»è®°å¿†ä¸­å®šä½æœç´¢æ¡†...")
        search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"],
                                         explicit_wait_timeout)

    if not search_box:
        print("ğŸ¤” è®°å¿†å¤±æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œå¯åŠ¨å¤§æ¨¡å‹æ™ºèƒ½å‘ç°æ¨¡å¼...")
        html = driver.page_source
        search_box_locator = get_locator_from_ai(html, "the main search input box for table names", api_key, base_url,
                                                 model_name)
        if search_box_locator:
            print(f"ğŸ¤– AIå‘ç°æœç´¢æ¡†å®šä½å™¨: {search_box_locator}")
            cache["search_box"] = search_box_locator
            save_json_file(cache, element_cache_file)
            search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"],
                                             explicit_wait_timeout)
        else:
            print("âŒ æ™ºèƒ½å‘ç°å¤±è´¥ï¼Œæ— æ³•æ‰¾åˆ°æœç´¢æ¡†ã€‚")
            return

    if not search_box:
        print("âŒ æœ€ç»ˆæœªèƒ½å®šä½åˆ°æœç´¢æ¡†ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
        return

    print("âœ… æˆåŠŸå®šä½åˆ°æœç´¢æ¡†ã€‚")

    for table_name in table_to_search:
        print(f"å¼€å§‹æŸ¥æ‰¾è¡¨ï¼š{table_name}")
        search_box.clear()
        search_box.send_keys(table_name)
        search_box.send_keys(Keys.ENTER)

        print(f"æ­£åœ¨æœç´¢ '{table_name}' çš„é“¾æ¥...")
        try:
            wait = WebDriverWait(driver, explicit_wait_timeout)
            # å®šä½åˆ°æˆ‘ä»¬æƒ³è¦ç‚¹å‡»çš„é‚£ä¸ªç²¾ç¡®çš„é“¾æ¥
            link_xpath = f"//a[contains(@href, '/tableShow/') and translate(normalize-space(.), 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz') = '{table_name.lower()}']"
            link_element = wait.until(EC.presence_of_element_located((By.XPATH, link_xpath)))

            # --- ç›´æ¥æå–hrefå¹¶å¯¼èˆª ---
            target_url = link_element.get_attribute('href')
            print(f"âœ… æˆåŠŸå®šä½é“¾æ¥ï¼Œæå–åˆ°ç›®æ ‡URL: {target_url}")

            if not target_url:
                print("âŒ æ— æ³•ä»é“¾æ¥å…ƒç´ ä¸­æå–åˆ°hrefå±æ€§ï¼Œç¨‹åºç»ˆæ­¢ã€‚")
                continue  # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨

            print("ğŸš€ æ­£åœ¨ç›´æ¥å¯¼èˆªåˆ°ç›®æ ‡URL...")
            driver.get(target_url)

            # ç›´æ¥å¯¼èˆªåï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦ç­‰å¾…æ–°è§†å›¾çš„æ ‡å¿—æ€§å…ƒç´ å‡ºç°
            print("â³ æ­£åœ¨ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½...")
            details_page_identifier = (By.CSS_SELECTOR, "table.table-column.table-interval-bg")
            wait.until(EC.presence_of_element_located(details_page_identifier))
            print("âœ… è¯¦æƒ…é¡µåŠ è½½æˆåŠŸã€‚")

            # ç°åœ¨å¯ä»¥å®‰å…¨åœ°è°ƒç”¨è¯¦æƒ…é¡µå¤„ç†å‡½æ•°
            process_table_details_page(driver, table_name, explicit_wait_timeout, output_json_file, api_key, base_url,
                                       model_name, min_sleep_time, max_sleep_time)
            print(f"--- è¡¨ '{table_name}' æ•°æ®æå–æµç¨‹ç»“æŸ ---")

        except Exception as e:
            print(f"âš ï¸ åœ¨æŸ¥æ‰¾ã€å¯¼èˆªæˆ–å¤„ç† '{table_name}' æ—¶å¤±è´¥: {e}")
            print(traceback.format_exc())
            # ä¿å­˜å¤±è´¥æ—¶çš„é¡µé¢å¿«ç…§ï¼Œä»¥ä¾¿è°ƒè¯•
            debug_file = f"debug_page_source_final_{table_name}.html"
            with open(debug_file, "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            print(f"â„¹ï¸ ä¸ºäº†ä¾¿äºè°ƒè¯•ï¼Œæœ€ç»ˆå¤±è´¥æ—¶çš„é¡µé¢HTMLå·²ä¿å­˜åˆ°: {os.path.abspath(debug_file)}")
            print(traceback.format_exc())
        finally:
            # æ¯æ¬¡å¤„ç†å®Œä¸€ä¸ªè¡¨åï¼Œè¿”å›åˆ°æœç´¢é¡µé¢ï¼Œä»¥ä¾¿æœç´¢ä¸‹ä¸€ä¸ªè¡¨
            driver.get(login_url)
            # é‡æ–°å®šä½æœç´¢æ¡†ï¼Œå› ä¸ºé¡µé¢å¯èƒ½åˆ·æ–°äº†
            search_box = find_element_safely(driver, search_box_locator["by"], search_box_locator["value"],
                                             explicit_wait_timeout)
            if not search_box:
                print("âŒ é‡æ–°å®šä½æœç´¢æ¡†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­å¤„ç†åç»­è¡¨ã€‚")
                break  # é€€å‡ºå¾ªç¯
            random_sleep(min_sleep_time, max_sleep_time)


def main(args):
    caffeinate_process = None
    if sys.platform == "darwin":
        print("ï£¿ æ£€æµ‹åˆ°macOSï¼Œå¯åŠ¨caffeinateå‘½ä»¤é˜²æ­¢ç³»ç»Ÿä¼‘çœ ...")
        caffeinate_process = subprocess.Popen(["caffeinate", "-d", "-i", "-m", "-s"])

    login_url = "https://dd.gildata.com/#/login"
    window_size = (1920, 1080)
    explicit_wait_timeout = 20
    final_observe_time = 5
    min_sleep_time = 0.5
    max_sleep_time = 2.5
    output_json_file = "table_definitions.json"

    driver = None
    try:
        driver = launch_browser(window_size)
        login_and_search(driver, args.login_wait_time, login_url, explicit_wait_timeout, args.tables, args.api_key,
                         args.base_url, args.model_name, min_sleep_time, max_sleep_time)
        print(f"æ“ä½œå®Œæˆï¼Œé¡µé¢å°†ä¿æŒæ‰“å¼€{final_observe_time} ç§’... ")
        time.sleep(final_observe_time)
    except Exception as e:
        print(f"ç¨‹åºå‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        print(traceback.format_exc())
    finally:
        if driver:
            print("å…³é—­æµè§ˆå™¨ã€‚")
            driver.quit()

        if caffeinate_process:
            print("ï£¿ æ­£åœ¨ç»ˆæ­¢caffeinateè¿›ç¨‹...")
            caffeinate_process.terminate()
            caffeinate_process.wait()
            print("âœ… caffeinateè¿›ç¨‹å·²ç»ˆæ­¢ï¼Œç³»ç»Ÿå¯æ­£å¸¸ä¼‘çœ ã€‚")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="çˆ¬å–æŒ‡å®šæ•°æ®è¡¨çš„è¯¦ç»†ä¿¡æ¯ã€‚")
    parser.add_argument("--tables", type=str, required=True, help="è¦çˆ¬å–çš„è¡¨åï¼Œå¤šä¸ªè¡¨ç”¨é€—å·åˆ†éš”ã€‚")
    parser.add_argument("--api_key", type=str, help="OpenAI API Key")
    parser.add_argument("--base_url", type=str, help="OpenAI API Base URL")
    parser.add_argument("--model_name", type=str, help="LLM æ¨¡å‹åç§°")
    parser.add_argument("--login_wait_time", type=int, default=60, help="ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•çš„æ—¶é—´ï¼ˆç§’ï¼‰ã€‚")

    args = parser.parse_args()
    args.tables = [table.strip() for table in args.tables.split(',')]

    main(args)
