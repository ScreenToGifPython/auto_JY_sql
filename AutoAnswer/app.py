import os
import sys
import json
import csv
import subprocess
import traceback
from typing import Tuple, List
from collections import defaultdict

import gradio as gr
import faiss
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer

try:
    import sqlglot
    from sqlglot.expressions import Table, Column, EQ
except ImportError:
    print("è­¦å‘Š: æœªå®‰è£… 'sqlglot' åº“ï¼ŒSQLå…³ç³»åˆ†æåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")
    print("è¯·é€šè¿‡ 'pip install sqlglot' å‘½ä»¤å®‰è£…ã€‚")
    sqlglot = None

# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_FILE = "llm_config.json"

# å®šä¹‰è„šæœ¬ç›®å½•
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

# å®šä¹‰é»˜è®¤å€¼
QUESTION = "å…¬å‹ŸåŸºé‡‘çš„äºŒçº§åˆ†ç±»åŸºé‡‘ç±»å‹æ˜¯è‚¡ç¥¨å‹çš„æœ€è¿‘1å¹´å‡€å€¼å’Œæ”¶ç›Šç‡æ•°æ®,åªè¦äº¤æ˜“æ—¥çš„æ•°æ®,ç”¨ä¸Šæµ·å¸‚åœºäº¤æ˜“æ—¥?"
DEFAULT_EMBED_MODEL_PATH = "BAAI/bge-m3"

# è¡¨ä¿¡æ¯æ–‡ä»¶è·¯å¾„
TABLE_JSON_PATH = os.path.join(SCRIPT_DIR, "table.json")
_TABLE_DATA = None


# --- Data Preprocessing Functions (Integrated) ---

def parse_csv_to_json(csv_file_path, json_file_path):
    tables_dict = {}
    current_table_name = None
    current_table_data = None
    in_fields_section = False
    with open(csv_file_path, 'r', encoding='utf-8-sig') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            if not any(field.strip() for field in row):
                if in_fields_section:
                    in_fields_section = False
                    if current_table_name and current_table_data:
                        tables_dict[current_table_name] = current_table_data
                        current_table_name = None
                        current_table_data = None
                continue
            if row and row[0].strip() == 'è¡¨å':
                if current_table_name and current_table_data:
                    tables_dict[current_table_name] = current_table_data
                table_name = row[1].strip() if len(row) > 1 else ''
                if table_name:
                    current_table_name = table_name
                    current_table_data = {'description': '', 'fields': []}
                in_fields_section = False
                continue
            if current_table_data:
                if row and row[0].strip() == 'æè¿°':
                    current_table_data['description'] = row[1].strip() if len(row) > 1 else ''
                    continue
                if row and row[0].strip() == 'å­—æ®µæ˜ç»†':
                    in_fields_section = True
                    continue
                if in_fields_section and len(row) > 5 and row[1].strip():
                    field_info = {
                        'field_name': row[1].strip(), 'field_type': row[2].strip(),
                        'is_nullable': row[3].strip(), 'default_value': row[4].strip(),
                        'field_description': row[5].strip()
                    }
                    current_table_data['fields'].append(field_info)
    if current_table_name and current_table_data:
        tables_dict[current_table_name] = current_table_data
    with open(json_file_path, 'w', encoding='utf-8') as jsonfile:
        json.dump(tables_dict, jsonfile, ensure_ascii=False, indent=4)


def extract_data_to_json(csv_file_path, output_json_path):
    results_dict = {}
    chunk_iter = pd.read_csv(csv_file_path, chunksize=10000, on_bad_lines='skip', low_memory=False)
    required_columns = ['id', 'req_url', 'db_sql']
    for i, chunk in enumerate(chunk_iter):
        if not all(col in chunk.columns for col in required_columns):
            raise ValueError("é”™è¯¯: CSVæ–‡ä»¶ä¸­ç¼ºå°‘å¿…éœ€çš„åˆ— (id, req_url, db_sql)ã€‚")
        chunk.dropna(subset=['id'], inplace=True)
        chunk['id'] = chunk['id'].astype(str)
        for index, row in chunk.iterrows():
            results_dict[row['id']] = {'req_url': row.get('req_url'), 'db_sql': row.get('db_sql')}
    with open(output_json_path, 'w', encoding='utf-8') as jsonfile:
        json.dump(results_dict, jsonfile, ensure_ascii=False, indent=4)


def analyze_sql_relationships(input_json_path, output_json_path, dialect="oracle"):
    if not sqlglot:
        raise ImportError("sqlglotåº“æœªå®‰è£…ï¼Œæ— æ³•åˆ†æSQLå…³ç³»ã€‚")
    with open(input_json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    table_relationships = defaultdict(lambda: defaultdict(set))
    for key, value in data.items():
        sql_query = value.get('db_sql')
        if not sql_query or not isinstance(sql_query, str): continue
        try:
            expr = sqlglot.parse_one(sql_query, read=dialect)
            if not expr: continue
            alias_map = {(t.alias_or_name or t.this.sql(dialect=dialect)).upper(): t.this.sql(dialect=dialect).upper()
                         for t in expr.find_all(Table)}
            for eq in expr.find_all(EQ):
                if not (isinstance(eq.left, Column) and isinstance(eq.right, Column)): continue
                l_table, r_table = alias_map.get((eq.left.table or "").upper()), alias_map.get(
                    (eq.right.table or "").upper())
                if not (l_table and r_table and l_table != r_table): continue
                l_col, r_col = eq.left.name.upper(), eq.right.name.upper()
                if (l_table, l_col) > (r_table, r_col):
                    l_table, r_table, l_col, r_col = r_table, l_table, r_col, l_col
                canonical = f"{l_table}.{l_col} = {r_table}.{r_col}"
                table_relationships[l_table][r_table].add(canonical)
        except Exception:
            pass
    final_relationships = defaultdict(dict)
    for table1, related_tables in table_relationships.items():
        for table2, conditions in related_tables.items():
            if table1 != table2:
                final_relationships[table1][table2] = sorted(list(conditions))
    with open(output_json_path, 'w', encoding='utf-8') as jsonfile:
        json.dump(final_relationships, jsonfile, ensure_ascii=False, indent=4)


def merge_and_govern_relations(info_path, relation_path, output_path):
    with open(info_path, 'r', encoding='utf-8') as f:
        table_info = json.load(f)
    with open(relation_path, 'r', encoding='utf-8') as f:
        table_relations = json.load(f)
    merged_data = {name: {"description": info.get("description", ""), "fields": info.get("fields", []),
                          "relations": table_relations.get(name, {})} for name, info in table_info.items()}
    for name, rel in table_relations.items():
        if name not in merged_data:
            merged_data[name] = {"description": "", "fields": [], "relations": rel}
    canonical_relations = {tuple(sorted([p.strip() for p in c.split('=')])) for d in merged_data.values() for cs in
                           d.get("relations", {}).values() for c in cs if len(c.split('=')) == 2}
    governed_relations = defaultdict(lambda: defaultdict(list))
    for rel_tuple in canonical_relations:
        p1, p2 = rel_tuple
        t1, t2 = p1.split('.')[0], p2.split('.')[0]
        if t1 in merged_data and t2 in merged_data:
            governed_relations[t1][t2].append(f"{p1} = {p2}")
            governed_relations[t2][t1].append(f"{p2} = {p1}")
    for name, relations in governed_relations.items():
        if name in merged_data:
            merged_data[name]["relations"] = dict(relations)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=4)


def create_text_chunks_from_json(src_json: dict) -> Tuple[List[str], List[str]]:
    table_names, text_chunks = [], []
    for tbl, info in src_json.items():
        if not isinstance(info, dict): continue
        desc = info.get("description") or info.get("tableChiName", "")
        fields, relations = info.get("fields", []), info.get("relations", {})
        block = [f"è¡¨å: {tbl}", f"è¡¨å«ä¹‰: {desc or 'N/A'}", "å­—æ®µ:"]
        block.extend([
                         f"  - å­—æ®µå:{f.get('field_name', '')}, ç±»å‹:{f.get('field_type', '')}, å«ä¹‰:{f.get('field_description', '')}"
                         for f in fields] if fields else ["  (æ— å­—æ®µä¿¡æ¯)"])
        block.append("ä¸å…¶ä»–è¡¨çš„å…³è”å…³ç³»:")
        block.extend([f"  - {c}" for tgt, conds in relations.items() for c in conds] if relations else [
            "ä¸å…¶ä»–è¡¨çš„å…³è”å…³ç³»: (æ— å…³è”ä¿¡æ¯)"])
        table_names.append(tbl)
        text_chunks.append("\n".join(block))
    return table_names, text_chunks


def run_vectorization(model_path, json_path, index_path, map_path, index_type, batch, max_len, progress=gr.Progress()):
    progress(0, desc="è¯»å–JSON...")
    with open(json_path, 'r', encoding='utf-8') as f:
        table_defs = json.load(f)
    progress(0.1, desc="ç”Ÿæˆæ–‡æœ¬å—...")
    _, chunks = create_text_chunks_from_json(table_defs)
    if not chunks: raise ValueError("æ— æ–‡æœ¬å—ç”Ÿæˆï¼Œé€€å‡º")
    progress(0.2, desc="åŠ è½½åµŒå…¥æ¨¡å‹...")
    model = SentenceTransformer(model_path, trust_remote_code=True)
    model.max_seq_length = max_len
    progress(0.4, desc="ç¼–ç å‘é‡...")
    vecs = model.encode(chunks, batch_size=batch, show_progress_bar=True, normalize_embeddings=True,
                        convert_to_numpy=True).astype("float32")
    progress(0.8, desc=f"æ„å»º {index_type} ç´¢å¼•...")
    dim = vecs.shape[1]
    if index_type == "flat":
        index = faiss.IndexFlatL2(dim)
        index.add(vecs)
    elif index_type == "hnsw":
        index = faiss.IndexHNSWFlat(dim, 32, faiss.METRIC_L2)
        index.hnsw.efConstruction, index.hnsw.efSearch = 200, 64
        index.add(vecs)
    else:
        raise ValueError("ç´¢å¼•ç±»å‹ä»…æ”¯æŒ flat / hnsw")
    faiss.write_index(index, index_path)
    progress(0.9, desc="ä¿å­˜FAISSç´¢å¼•...")
    with open(map_path, "w", encoding="utf-8") as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)
    progress(1, desc="ä¿å­˜è¡¨ç»“æ„æ˜ å°„...")


def run_preprocessing_pipeline(table_struct_csv, segment_sql_csv, sql_dialect, embed_model, faiss_type, batch_size,
                               max_len, progress=gr.Progress()):
    log_output = ""

    def log(msg):
        nonlocal log_output; log_output += msg + "\n"; return log_output

    try:
        yield log("ã€æ­¥éª¤ 1/5ã€‘ æ­£åœ¨è½¬æ¢è¡¨ç»“æ„ CSV -> JSON...")
        table_info_json = os.path.join(SCRIPT_DIR, "table_info.json")
        parse_csv_to_json(table_struct_csv, table_info_json)
        yield log("ã€æ­¥éª¤ 1/5ã€‘ âœ… æˆåŠŸç”Ÿæˆ table_info.json")

        yield log("\nã€æ­¥éª¤ 2/5ã€‘ æ­£åœ¨ä» SQL æ—¥å¿—æå–æ•°æ®...")
        extracted_data_json = os.path.join(SCRIPT_DIR, "extracted_data.json")
        extract_data_to_json(segment_sql_csv, extracted_data_json)
        yield log("ã€æ­¥éª¤ 2/5ã€‘ âœ… æˆåŠŸç”Ÿæˆ extracted_data.json")

        yield log(f"\nã€æ­¥éª¤ 3/5ã€‘ æ­£åœ¨ä½¿ç”¨ {sql_dialect} æ–¹è¨€åˆ†æ SQL å…³ç³»...")
        table_relation_json = os.path.join(SCRIPT_DIR, "table_relation.json")
        analyze_sql_relationships(extracted_data_json, table_relation_json, dialect=sql_dialect)
        yield log("ã€æ­¥éª¤ 3/5ã€‘ âœ… æˆåŠŸç”Ÿæˆ table_relation.json")

        yield log("\nã€æ­¥éª¤ 4/5ã€‘ æ­£åœ¨åˆå¹¶è¡¨ä¿¡æ¯ä¸å…³ç³»...")
        final_table_json = os.path.join(SCRIPT_DIR, "table.json")
        merge_and_govern_relations(table_info_json, table_relation_json, final_table_json)
        yield log("ã€æ­¥éª¤ 4/5ã€‘ âœ… æˆåŠŸç”Ÿæˆ table.json")

        yield log("\nã€æ­¥éª¤ 5/5ã€‘ æ­£åœ¨å‘é‡åŒ–è¡¨ç»“æ„...")
        faiss_index_bin = os.path.join(SCRIPT_DIR, "faiss_index.bin")
        table_mapping_json = os.path.join(SCRIPT_DIR, "table_mapping.json")
        run_vectorization(embed_model, final_table_json, faiss_index_bin, table_mapping_json, faiss_type,
                          int(batch_size), int(max_len), progress)
        yield log("ã€æ­¥éª¤ 5/5ã€‘ âœ… æˆåŠŸç”Ÿæˆ FAISS ç´¢å¼•å’Œæ˜ å°„æ–‡ä»¶ã€‚")

        yield log("\nğŸ‰ æ‰€æœ‰é¢„å¤„ç†æ­¥éª¤å®Œæˆï¼")
    except Exception as e:
        yield log(f"âŒ å¤„ç†å¤±è´¥: {e}\n{traceback.format_exc()}")


# --- Core App Functions ---

def load_table_data():
    global _TABLE_DATA
    if _TABLE_DATA is None:
        try:
            with open(TABLE_JSON_PATH, 'r', encoding='utf-8') as f:
                _TABLE_DATA = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            _TABLE_DATA = {}
    return _TABLE_DATA


def save_config(embed_model_path, top_k, llm_model, api_key, llm_url, sql_type):
    config = {"EMBED_MODEL": embed_model_path, "TOP_K": top_k, "LLM_MODEL": llm_model, "API_KEY": api_key,
              "LLM_URL": llm_url, "SQL_TYPE": sql_type}
    with open(CONFIG_FILE, 'w', encoding='utf-8') as f: json.dump(config, f, indent=4)
    return f"âœ… é…ç½®æˆåŠŸä¿å­˜åˆ° {CONFIG_FILE}"


def load_config():
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {}
    return {}


def format_table_details(table_name: str):
    table_info = load_table_data().get(table_name.strip().upper())
    if not table_info: return f"âŒ æœªæ‰¾åˆ°è¡¨: **{table_name}**"
    md = f"## è¡¨å: {table_name}\n**è¡¨å«ä¹‰**: {table_info.get('description', 'æ— ')}\n\n### å­—æ®µä¿¡æ¯:\n"
    if table_info.get('fields'):
        md += "| å­—æ®µå | å­—æ®µå«ä¹‰ | å­—æ®µæ ¼å¼ | å¯å¦ä¸ºç©º | é»˜è®¤å€¼ |\n|---|---|---|---|---|\n"
        md += "\n".join([
                            f"| {f.get('field_name', '')} | {f.get('field_description', '')} | {f.get('field_type', '')} | {f.get('is_nullable', '')} | {f.get('default_value', '')} |"
                            for f in table_info['fields']])
    else:
        md += "æ— å­—æ®µä¿¡æ¯ã€‚\n"
    md += "\n### å…³è”ä¿¡æ¯:\n"
    if table_info.get('relations'):
        md += "\n".join(
            [f"- **{rel_table}**: ` {cond} `" for rel_table, conds in table_info['relations'].items() for cond in
             conds])
    else:
        md += "æ— å…³è”ä¿¡æ¯ã€‚\n"
    return md


def generate_sql_and_log(question, embed_model_path, top_k, llm_model, api_key, llm_url, sql_type):
    if not all([question, embed_model_path, llm_model, api_key, llm_url, sql_type]):
        yield "âŒ é”™è¯¯: æ‰€æœ‰å­—æ®µå‡ä¸èƒ½ä¸ºç©º.", ""
        return
    command = [sys.executable, os.path.join(SCRIPT_DIR, "rag_query.py"), "--question", question, "--embed_model_path",
               embed_model_path, "--k", str(int(top_k)), "--key", api_key, "--url", llm_url, "--sql_type", sql_type]
    log_content, sql_content = f"â–¶ï¸ æ‰§è¡Œå‘½ä»¤: {' '.join(command)}\n" + "-" * 20 + "\n", "â³ ç­‰å¾…è„šæœ¬æ‰§è¡Œ..."
    yield log_content, sql_content
    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, encoding='utf-8',
                               bufsize=1, universal_newlines=True, cwd=SCRIPT_DIR)
    for line in iter(process.stdout.readline, ''):
        log_content += line
        yield log_content, sql_content
    process.stdout.close()
    return_code = process.wait()
    if return_code != 0:
        log_content += f"\n--- è„šæœ¬æ‰§è¡Œå‡ºé”™, è¿”å›ç : {return_code} ---\n"
        sql_content = "âŒ æ‰§è¡Œå¤±è´¥"
    else:
        log_content += "\n--- è„šæœ¬æ‰§è¡Œå®Œæ¯• ---\n"
        try:
            start_tag, end_tag = "```sql", "```"
            start_index = log_content.rfind(start_tag)
            if start_index != -1:
                end_index = log_content.find(end_tag, start_index + len(start_tag))
                sql_content = log_content[
                              start_index + len(start_tag):end_index].strip() if end_index != -1 else "âŒ æœªæ‰¾åˆ°ç»“æŸæ ‡è®°"
            else:
                sql_content = "âŒ æœªæ‰¾åˆ°å¼€å§‹æ ‡è®°"
        except Exception as e:
            sql_content = f"âŒ è§£æSQLæ—¶å‡ºé”™: {e}"
    yield log_content, sql_content


# --- Gradio UI ---
def create_ui():
    loaded_config = load_config()
    with gr.Blocks(title="RAG SQL Generator", theme=gr.themes.Soft()) as demo:
        gr.Markdown("## ğŸ“ RAG-based SQL Generator")
        gr.Markdown("é€šè¿‡è¾“å…¥è‡ªç„¶è¯­è¨€é—®é¢˜, æ£€ç´¢ç›¸å…³åº“è¡¨ç»“æ„, å¹¶ç”±å¤§æ¨¡å‹ç”Ÿæˆç›¸åº”çš„ SQL æŸ¥è¯¢.")

        with gr.Tab("å¤§æ¨¡å‹SQLç”Ÿæˆ"):
            with gr.Row():
                with gr.Column(scale=3):
                    question_input = gr.Textbox(lines=8, label="ç”¨æˆ·é—®é¢˜", placeholder=f"ä¾‹å¦‚: '{QUESTION}'")
                    sql_result_output = gr.Code(label="SQL ç”Ÿæˆç»“æœ", language="sql", lines=10, interactive=False)
                    log_output = gr.Textbox(lines=10, label="è¯¦ç»†æ—¥å¿—", interactive=False)
                with gr.Column(scale=1):
                    gr.Markdown("#### âš™ï¸ å‚æ•°é…ç½®")
                    embed_model_input = gr.Textbox(value=loaded_config.get("EMBED_MODEL", ""), label="åµŒå…¥æ¨¡å‹è·¯å¾„",
                                                   interactive=True, placeholder="ä¾‹å¦‚: BAAI/bge-m3")
                    top_k_input = gr.Slider(minimum=1, maximum=50, value=loaded_config.get("TOP_K", 10), step=1,
                                            label="æ£€ç´¢ Top-K")
                    sql_type_input = gr.Dropdown(["MYSQL", "PostgreSQL", "SparkSQL", "Oracle", "SQLServer"],
                                                 value=loaded_config.get("SQL_TYPE", "MYSQL"), label="ç›®æ ‡ SQL ç±»å‹")
                    gr.Markdown("---")
                    gr.Markdown("#### ğŸ§  å¤§æ¨¡å‹é…ç½®")
                    llm_model_input = gr.Textbox(value=loaded_config.get("LLM_MODEL", ""), label="å¤§æ¨¡å‹åç§°")
                    api_key_input = gr.Textbox(value=loaded_config.get("API_KEY", ""), label="API å¯†é’¥",
                                               type="password")
                    llm_url_input = gr.Textbox(value=loaded_config.get("LLM_URL", ""), label="å¤§æ¨¡å‹æœåŠ¡åœ°å€")
                    save_button = gr.Button("ä¿å­˜é…ç½®")
                    generate_button = gr.Button("ğŸš€ ç”ŸæˆSQL", variant="primary")
            save_button.click(fn=save_config,
                              inputs=[embed_model_input, top_k_input, llm_model_input, api_key_input, llm_url_input,
                                      sql_type_input], outputs=[log_output])
            generate_button.click(fn=generate_sql_and_log,
                                  inputs=[question_input, embed_model_input, top_k_input, llm_model_input,
                                          api_key_input, llm_url_input, sql_type_input],
                                  outputs=[log_output, sql_result_output])

        with gr.Tab("æ•°æ®é¢„å¤„ç†"):
            gr.Markdown("## âš™ï¸ æ•°æ®é¢„å¤„ç†ä¸å‘é‡åŒ–")
            gr.Markdown("ä¸€é”®å®Œæˆä»åŸå§‹CSVåˆ°FAISSå‘é‡ç´¢å¼•çš„å®Œæ•´æµç¨‹ã€‚")
            with gr.Row():
                with gr.Column(scale=2):
                    gr.Markdown("#### è¾“å…¥æ–‡ä»¶è·¯å¾„")
                    table_struct_csv_input = gr.Textbox(label="è¡¨ç»“æ„CSVæ–‡ä»¶è·¯å¾„",
                                                        value=os.path.join(SCRIPT_DIR, "è¡¨ç»“æ„.csv"))
                    segment_sql_csv_input = gr.Textbox(label="SQLæ—¥å¿—CSVæ–‡ä»¶è·¯å¾„",
                                                       value=os.path.join(SCRIPT_DIR, "segment_sql.csv"))
                    gr.Markdown("#### å‚æ•°é…ç½®")
                    sql_dialect_input = gr.Dropdown(label="SQLæ–¹è¨€ (ç”¨äºå…³ç³»åˆ†æ)",
                                                    choices=["oracle", "mysql", "postgres", "spark"], value="mysql")
                    preprocess_embed_model_input = gr.Textbox(label="åµŒå…¥æ¨¡å‹ (è·¯å¾„æˆ–HuggingFaceåç§°)",
                                                              value=loaded_config.get("EMBED_MODEL",
                                                                                      DEFAULT_EMBED_MODEL_PATH))
                    faiss_type_input = gr.Dropdown(label="FAISSç´¢å¼•ç±»å‹", choices=["hnsw", "flat"], value="hnsw")
                    with gr.Row():
                        batch_size_input = gr.Number(label="æ‰¹å¤„ç†å¤§å°", value=16, minimum=1, precision=0)
                        max_len_input = gr.Number(label="æœ€å¤§åºåˆ—é•¿åº¦", value=512, minimum=1, precision=0)
                    preprocess_button = gr.Button("ğŸš€ å¼€å§‹ä¸€é”®é¢„å¤„ç†", variant="primary")
                with gr.Column(scale=3):
                    preprocess_log_output = gr.Textbox(label="å¤„ç†æ—¥å¿—", lines=22, interactive=False)
            preprocess_button.click(fn=run_preprocessing_pipeline,
                                    inputs=[table_struct_csv_input, segment_sql_csv_input, sql_dialect_input,
                                            preprocess_embed_model_input, faiss_type_input, batch_size_input,
                                            max_len_input], outputs=[preprocess_log_output])

        with gr.Tab("è¡¨ä¿¡æ¯æŸ¥è¯¢"):
            gr.Markdown("## ğŸ” è¡¨ä¿¡æ¯æŸ¥è¯¢")
            gr.Markdown("è¾“å…¥è¡¨åï¼ŒæŸ¥è¯¢å…¶è¯¦ç»†ç»“æ„ã€å«ä¹‰åŠå…³è”ä¿¡æ¯ã€‚")
            with gr.Row():
                with gr.Column(scale=1):
                    table_name_query_input = gr.Textbox(label="è¾“å…¥è¡¨å", placeholder="ä¾‹å¦‚: ADS_CODE_MAPPING")
                    query_table_button = gr.Button("æŸ¥è¯¢è¡¨ä¿¡æ¯")
                with gr.Column(scale=2):
                    table_details_output = gr.Markdown(label="è¡¨è¯¦ç»†ä¿¡æ¯")
            query_table_button.click(fn=format_table_details, inputs=[table_name_query_input],
                                     outputs=[table_details_output])
            table_name_query_input.submit(fn=format_table_details, inputs=[table_name_query_input],
                                          outputs=[table_details_output])

        with gr.Tab("è¯´æ˜æ–‡æ¡£"):
            gr.Markdown("""
                ## ğŸ“– ä½¿ç”¨è¯´æ˜
                ### å¤§æ¨¡å‹SQLç”Ÿæˆ
                åœ¨æ­¤é¡µé¢ï¼Œæ‚¨å¯ä»¥è¾“å…¥è‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œå¹¶é…ç½®ç›¸å…³å‚æ•°ï¼Œè°ƒç”¨å¤§æ¨¡å‹ç”ŸæˆSQLã€‚
                ### æ•°æ®é¢„å¤„ç†
                åœ¨æ­¤é¡µé¢ï¼Œæ‚¨å¯ä»¥ä¸€é”®å®Œæˆæ•°æ®é¢„å¤„ç†å’Œå‘é‡åŒ–ï¼Œä¸ºâ€œå¤§æ¨¡å‹SQLç”Ÿæˆâ€åšæ•°æ®å‡†å¤‡ã€‚
                ### è¡¨ä¿¡æ¯æŸ¥è¯¢
                åœ¨æ­¤é¡µé¢ï¼Œæ‚¨å¯ä»¥æŸ¥è¯¢å·²å¤„ç†å¥½çš„è¡¨çš„è¯¦ç»†ä¿¡æ¯ã€‚
                """)
    return demo


if __name__ == "__main__":
    ui = create_ui()
    ui.launch(server_name="0.0.0.0", server_port=7861)
