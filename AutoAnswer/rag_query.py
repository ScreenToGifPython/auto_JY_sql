#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
rag_sql_generator.py
ä½¿ç”¨ RAG æ£€ç´¢è¡¨ç»“æ„ â†’ è°ƒç”¨ LLM ç”Ÿæˆ SQL
Author: KevinChen
"""

import os, json, argparse
from typing import List
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI, OpenAIError
import backoff

# ----------------- é»˜è®¤å‚æ•° -----------------
EMBED_MODEL = "BAAI/bge-m3"
INDEX_PATH = "faiss_index.bin"
MAP_PATH = "table_mapping.json"
TABLE_JSON = "table.json"
TOP_K = 10
MAX_LEN = 512
BATCH_SIZE = 16
LLM_MODEL = "deepseek-chat"  # å¯æ¢å…¶ä»–
API_KEY = "sk-43a58ad2bbd740b095f5f61671ed9fae"
LLM_URL = "https://api.deepseek.com"
QUESTION = "æ€ä¹ˆæŸ¥æ‰¾å‘¨é¢‘ç§å‹ŸåŸºé‡‘çš„æ—¥æ¶¨è·Œæ•°æ®?"
# ----------------------------------------

# --------- ç¦ç”¨å¹¶è¡Œ tokenizer çº¿ç¨‹ï¼Œé¿å…è„šæœ¬ä¸é€€å‡º ----------
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ---------------- LLM è°ƒç”¨ ----------------
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€åèµ„æ·±æ•°æ®å·¥ç¨‹å¸ˆï¼Œç²¾é€š SQL ç¼–å†™ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. **å€¼æ˜ å°„ï¼š** è‹¥å­—æ®µå¤‡æ³¨å·²ç»™ä¸­æ–‡â†”ä»£ç æ˜ å°„ï¼Œè¯·å…ˆæŠŠç”¨æˆ·æè¿°è½¬æ¢ä¸ºå¯¹åº”ä»£ç å†è¿‡æ»¤ã€‚
2. **è¡¨é€‰æ‹©ï¼š** åªé€‰æ‹©ä¸éœ€æ±‚ç›´æ¥ç›¸å…³çš„è¡¨ï¼Œé¿å…å†—ä½™ JOINã€‚
3. **JOIN æ¡ä»¶ï¼š** å½“ç¡®æœ‰å…³ç³»æ—¶ä½¿ç”¨è¡¨é—´å…³è”å­—æ®µã€‚
4. **æ³¨é‡Šï¼š** SQL åŠ ä¸Šä¸­æ–‡æ³¨é‡Šè§£é‡Šå­—æ®µå«ä¹‰ã€è¿‡æ»¤æ¡ä»¶åŠ JOIN é€»è¾‘ã€‚
"""


@backoff.on_exception(backoff.expo, OpenAIError, max_tries=3)
def call_llm(prompt: str,
             api_key: str,
             base_url: str,
             model: str = LLM_MODEL,
             temperature: float = 0.1) -> str:
    client = OpenAI(api_key=api_key, base_url=base_url)
    resp = client.chat.completions.create(
        model=model,
        temperature=temperature,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ])
    return resp.choices[0].message.content.strip()


# ------------- æ„é€  prompt ----------------
def build_prompt(user_q: str, ctx_blocks: List[str]) -> str:
    ctx_txt = "\n\n--- ç›¸å…³è¡¨ç»“æ„ ---\n" + "\n\n".join(ctx_blocks)
    return f"""{ctx_txt}

--- ç”¨æˆ·éœ€æ±‚ ---
{user_q}

è¯·åœ¨ ```sql ``` å—ä¸­ç»™å‡ºæœ€ç»ˆ SQL, å¹¶åœ¨ä»£ç å—ä¸­ç”¨æ³¨é‡Šè§£é‡Šæ‰€ç”¨è¡¨ã€å­—æ®µã€JOIN é€»è¾‘ã€‚
"""


# ------------- ä¸»å‡½æ•° --------------------
def rag_sql(question: str,
            api_key: str,
            base_url: str,
            top_k: int = TOP_K) -> str:
    # 1. åŠ è½½èµ„æºï¼ˆå…¨å±€æ‡’åŠ è½½ï¼‰
    global faiss_index, id2text, embedder
    if "faiss_index" not in globals():
        faiss_index = faiss.read_index(INDEX_PATH)
        id2text = json.load(open(MAP_PATH, encoding="utf-8"))
        embedder = SentenceTransformer(EMBED_MODEL)
        embedder.max_seq_length = MAX_LEN

    # 2. ç¼–ç é—®é¢˜å‘é‡
    q_vec = embedder.encode([question], normalize_embeddings=True).astype("float32")

    # 3. æ£€ç´¢
    _, I = faiss_index.search(q_vec, top_k)
    ctx_blocks = [id2text[str(idx)] if isinstance(id2text, dict) else id2text[idx]
                  for idx in I[0]]
    print(f"\nğŸ¯ Top-{top_k} å‘½ä¸­è¡¨ç´¢å¼•: {I[0]}")
    for i, block in enumerate(ctx_blocks):
        print(f"\nâ€”â€” è¡¨ç»“æ„ Top-{i + 1} â€”â€”\n{block}")

    # 4. æ„é€  Prompt
    prompt = build_prompt(question, ctx_blocks)
    print(f"\nğŸ“¤ å‘é€ç»™å¤§æ¨¡å‹çš„Promptå¦‚ä¸‹ï¼š\n{prompt}\n")

    # 5. è°ƒç”¨ LLM
    return call_llm(prompt, api_key=api_key, base_url=base_url)


# ---------------- CLI ----------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RAG ç”Ÿæˆ SQL demo")
    parser.add_argument("--question", default=QUESTION, help="ç”¨æˆ·è‡ªç„¶è¯­è¨€é—®é¢˜")
    parser.add_argument("--k", type=int, default=TOP_K, help="æ£€ç´¢ top-k")
    parser.add_argument("--key", default=API_KEY, help="OpenAI API Key")
    parser.add_argument("--url", default=LLM_URL, help="OpenAI Base URL")
    args = parser.parse_args()

    answer = rag_sql(args.question, api_key=args.key, base_url=args.url, top_k=args.k)
    print("\n=== LLM å›å¤ ===\n")
    print(answer)
    os._exit(0)
