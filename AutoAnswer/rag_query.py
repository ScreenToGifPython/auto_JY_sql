#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
rag_sql_generator.py
ä½¿ç”¨ RAG æ£€ç´¢è¡¨ç»“æ„ 
è°ƒç”¨ LLM ç”Ÿæˆ SQL æˆ–ç›´æ¥è¿›è¡Œè¡¨æ£€ç´¢
Author: KevinChen
"""

import faiss
import backoff
import warnings
import requests
from typing import List
import os, json, argparse, sys
from openai import OpenAI, OpenAIError
from sentence_transformers import SentenceTransformer

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# ----------------- é»˜è®¤å‚æ•° -----------------
INDEX_PATH = "faiss_index.bin"
MAP_PATH = "table_mapping.json"
SQL_TYPE = "mysql"
TOP_K = 10
MAX_LEN = 512
LLM_MODEL = "deepseek-chat"
API_KEY = "sk-xxxxxxxx"
LLM_URL = "https://api.deepseek.com"
QUESTION = "å…¬å‹ŸåŸºé‡‘çš„äºŒçº§åˆ†ç±»åŸºé‡‘ç±»å‹æ˜¯è‚¡ç¥¨å‹çš„æœ€è¿‘1å¹´å‡€å€¼å’Œæ”¶ç›Šç‡æ•°æ®,åªè¦äº¤æ˜“æ—¥çš„æ•°æ®,ç”¨ä¸Šæµ·å¸‚åœºäº¤æ˜“æ—¥?"
DEFAULT_EMBED_MODEL_PATH = "BAAI/bge-m3"

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# å…¨å±€å˜é‡ç”¨äºç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½
_embedder_instance = None
_current_embed_model_path = None
_faiss_index = None
_id2text = None


def load_retrieval_resources(embed_model_path: str):
    """æ‡’åŠ è½½æˆ–é‡æ–°åŠ è½½ FAISS ç´¢å¼•ã€æ–‡æœ¬æ˜ å°„å’ŒåµŒå…¥æ¨¡å‹ã€‚"""
    global _faiss_index, _id2text, _embedder_instance, _current_embed_model_path

    if _faiss_index is None:
        print("é¦–æ¬¡åŠ è½½ FAISS ç´¢å¼•å’Œæ˜ å°„...", file=sys.stderr)
        _faiss_index = faiss.read_index(INDEX_PATH)
        _id2text = json.load(open(MAP_PATH, encoding="utf-8"))

    if _embedder_instance is None or _current_embed_model_path != embed_model_path:
        print(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {embed_model_path}...", file=sys.stderr)
        _embedder_instance = SentenceTransformer(embed_model_path)
        _embedder_instance.max_seq_length = MAX_LEN
        _current_embed_model_path = embed_model_path
        print("åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ.", file=sys.stderr)

    return _embedder_instance, _faiss_index, _id2text


@backoff.on_exception(backoff.expo, (OpenAIError, requests.exceptions.RequestException), max_tries=3)
def call_llm(prompt: str, api_key: str, base_url: str, sql_type: str, model: str = LLM_MODEL,
             temperature: float = 0.1) -> str:
    sys_prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±æ•°æ®å·¥ç¨‹å¸ˆï¼Œç²¾é€š{sql_type}æ•°æ®åº“çš„ SQL ç¼–å†™ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. **å€¼æ˜ å°„ï¼š** è‹¥å­—æ®µå¤‡æ³¨å·²ç»™ä¸­æ–‡ä»£ç æ˜ å°„ï¼Œè¯·å…ˆæŠŠç”¨æˆ·æè¿°è½¬æ¢ä¸ºå¯¹åº”ä»£ç å†è¿‡æ»¤ã€‚
2. **è¡¨é€‰æ‹©ï¼š** åªé€‰æ‹©ä¸éœ€æ±‚ç›´æ¥ç›¸å…³çš„è¡¨ï¼Œé¿å…å†—ä½™ JOINã€‚
3. **JOIN æ¡ä»¶ï¼š** å½“ç¡®æœ‰å…³ç³»æ—¶ä½¿ç”¨è¡¨é—´å…³è”å­—æ®µã€‚
4. **æ³¨é‡Šï¼š** SQL åŠ ä¸Šä¸­æ–‡æ³¨é‡Šè§£é‡Šå­—æ®µå«ä¹‰ã€è¿‡æ»¤æ¡ä»¶åŠ JOIN é€»è¾‘ã€‚
5. **æ‰§è¡Œæ•ˆç‡ï¼š** ä½ å†™çš„ SQL ä¸€å®šæ˜¯æ‰§è¡Œæ•ˆç‡æœ€é«˜çš„SQLä»£ç , ç»å¯¹ç¬¦åˆ{sql_type}æ•°æ®åº“çš„ç‰¹æ€§,è¯­æ³•,æ‰§è¡Œæ•ˆç‡ã€‚
6. **æé—®ï¼š** å¦‚æœä½ è®¤ä¸ºç”¨æˆ·çš„é—®é¢˜å¤ªæ¨¡ç³Š,æˆ–è€…ä½ éœ€è¦æ›´å¤šçš„ä¿¡æ¯,è¯·åœ¨ç”ŸæˆSQLä¹‹ååŠ å†ä»¥æ³¨é‡Šå½¢å¼è¯´æ˜ã€‚å¯ä»¥è¦æ±‚ç”¨æˆ·æä¾›æ›´å¤šä¿¡æ¯å¹¶ä¸” "æé«˜æ£€ç´¢Top-K"ã€‚è¯´æ˜ä½ éœ€è¦ä»€ä¹ˆä¿¡æ¯ã€‚
6. **è§£é‡Šæ¨¡æ‹Ÿï¼š** å¦‚æœç”ŸæˆSQLçš„è¡¨å’Œå­—æ®µæ•°æ®ä¸åœ¨æä¾›çš„è¡¨ç»“æ„ä¸­ (æ˜¯ä½ æ¨¡æ‹Ÿçš„), è¯·åœ¨SQLä»£ç å—ä¸­ç”¨æ³¨é‡Šå¼ºè°ƒè¯´æ˜ã€‚
    """
    if "lightcode-ui" in base_url:
        headers = {'Accept': "*/*", 'Authorization': f"Bearer {api_key}", 'Content-Type': "application/json"}
        payload = {"model": "gpt-4o",
                   "messages": [{"role": "system", "content": sys_prompt}, {"role": "user", "content": prompt}],
                   "stream": False}
        response = requests.post(base_url, headers=headers, data=json.dumps(payload))
        if response.ok:
            result = response.json()
            if 'choices' in result and result.get('choices'):
                message = result['choices'][0].get('message', {})
                if 'content' in message:
                    return message['content'].strip()
            raise KeyError(f"æ— æ³•ä»å“åº”ä¸­æå–å†…å®¹ã€‚æ”¶åˆ°çš„å“åº”: {json.dumps(result, ensure_ascii=False)}")
        else:
            response.raise_for_status()
    else:
        client = OpenAI(api_key=api_key, base_url=base_url)
        resp = client.chat.completions.create(model=model, temperature=temperature,
                                              messages=[{"role": "system", "content": sys_prompt},
                                                        {"role": "user", "content": prompt}])
        return resp.choices[0].message.content.strip()


def build_prompt(user_q: str, sql_type: str, ctx_blocks: List[str]) -> str:
    ctx_txt = "\n\n--- ç›¸å…³è¡¨ç»“æ„ ---\n" + "\n\n".join(ctx_blocks)
    return f"""{ctx_txt}

--- ç”¨æˆ·éœ€æ±‚ ---
{user_q}

è¯·åœ¨ ```sql ``` å—ä¸­ç»™å‡ºæœ€ç»ˆç¬¦åˆ{sql_type}è¯­æ³•çš„ SQL, è¦æœ‰è¯¦ç»†çš„å­—æ®µæ³¨é‡Š,å…³è”æ³¨é‡Š,è¡¨å«ä¹‰æ•°ç»„, ä»¥åŠæ¡ä»¶æ³¨é‡Šã€‚
å¹¶åœ¨ä»£ç å—ä¸­ç”¨æ³¨é‡Šè§£é‡Šæ‰€ç”¨è¡¨ã€å­—æ®µã€JOIN é€»è¾‘ã€‚å¯¹äºä½ ä¸çŸ¥é“çš„å­—æ®µå€¼æ˜ å°„å…³ç³»,ä½ è¦è¿›è¡Œè¯´æ˜ã€‚
è¯·å‹¿è¿”å›å…¶ä»–å†…å®¹ï¼Œåªè¿”å› ```sql ``` å—ã€‚è¯·æ³¨æ„SQLçš„æ‰§è¡Œæ•ˆç‡,å†™å‡ºæœ€ä½³æ€§èƒ½çš„sqlä»£ç ã€‚
å¦‚æœéœ€æ±‚è¿‡äºæ¨¡ç³Š, åœ¨è¿”å›SQLåç”¨æ³¨é‡Šè¯´æ˜ä½ éœ€è¦ä»€ä¹ˆä¿¡æ¯ã€‚ å¦‚æœä½ éœ€è¦æ›´å¤šçš„è¡¨ç»“æ„, åœ¨è¿”å›SQLåç”¨æ³¨é‡Šå‘Šè¯‰ç”¨æˆ· 'æé«˜æ£€ç´¢Top-K'ã€‚
å¦‚æœç”ŸæˆSQLçš„è¡¨å’Œå­—æ®µæ•°æ®ä¸åœ¨æä¾›çš„è¡¨ç»“æ„ä¸­ (æ˜¯ä½ æ¨¡æ‹Ÿçš„), è¯·åœ¨SQLä»£ç å—ä¸­ç”¨æ³¨é‡Šå¼ºè°ƒè¯´æ˜ã€‚
    """


def rag_sql_generator(question: str, api_key: str, base_url: str, sql_type: str, embed_model_path: str, top_k: int):
    embedder, faiss_index, id2text = load_retrieval_resources(embed_model_path)
    q_vec = embedder.encode([question], normalize_embeddings=True).astype("float32")
    _, I = faiss_index.search(q_vec, top_k)
    ctx_blocks = [id2text[idx] for idx in I[0]]
    yield f"ğŸ¯ Top-{top_k} å‘½ä¸­è¡¨ç´¢å¼•: {I[0]}"
    for i, block in enumerate(ctx_blocks):
        yield f"â€”â€” è¡¨ç»“æ„ Top-{i + 1} â€”â€”\n{block}"
    prompt = build_prompt(question, sql_type, ctx_blocks)
    yield f"ğŸ“¤ å‘é€ç»™å¤§æ¨¡å‹çš„Promptå¦‚ä¸‹ï¼š\n{prompt}"
    final_answer = call_llm(prompt, api_key=api_key, base_url=base_url, sql_type=sql_type)
    yield "=== LLM å›å¤ ==="
    yield final_answer


def find_similar_tables(question: str, embed_model_path: str, top_k: int):
    embedder, faiss_index, id2text = load_retrieval_resources(embed_model_path)
    q_vec = embedder.encode([question], normalize_embeddings=True).astype("float32")
    distances, indices = faiss_index.search(q_vec, top_k)
    results = []
    for i in range(top_k):
        idx = indices[0][i]
        dist = distances[0][i]
        similarity = max(0, 1 - (dist ** 2) / 2)
        details = id2text[idx]
        table_name = "æœªçŸ¥è¡¨"
        for line in details.split('\n'):
            if line.startswith("è¡¨å:"):
                table_name = line.replace("è¡¨å:", "").strip()
                break
        results.append({
            "table_name": table_name,
            "similarity_percentage": round(similarity * 100, 2),
            "details": details
        })
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RAG SQL Agent")
    parser.add_argument("--question", default=QUESTION, help="ç”¨æˆ·è‡ªç„¶è¯­è¨€é—®é¢˜")
    parser.add_argument("--k", type=int, default=TOP_K, help="æ£€ç´¢ top-k")
    parser.add_argument("--embed_model_path", type=str, default=DEFAULT_EMBED_MODEL_PATH, help="åµŒå…¥æ¨¡å‹è·¯å¾„")
    parser.add_argument("--mode", type=str, default="sql", choices=["sql", "search"],
                        help="æ‰§è¡Œæ¨¡å¼: 'sql' (ç”ŸæˆSQL) æˆ– 'search' (è¡¨æ£€ç´¢)")
    # 'sql' mode specific arguments
    parser.add_argument("--key", type=str, default=API_KEY, help="LLM API Key")
    parser.add_argument("--url", type=str, default=LLM_URL, help="LLM Base URL")
    parser.add_argument("--sql_type", type=str, default=SQL_TYPE, help="ç›®æ ‡SQLæ–¹è¨€")
    args = parser.parse_args()

    if args.mode == 'sql':
        for message in rag_sql_generator(question=args.question, api_key=args.key, base_url=args.url,
                                         sql_type=args.sql_type, embed_model_path=args.embed_model_path, top_k=args.k):
            print(message)
    elif args.mode == 'search':
        search_results = find_similar_tables(question=args.question, embed_model_path=args.embed_model_path,
                                             top_k=args.k)
        print(json.dumps(search_results, ensure_ascii=False, indent=2))

    os._exit(0)
