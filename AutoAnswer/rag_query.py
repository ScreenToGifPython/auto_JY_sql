#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
rag_sql_generator.py
ä½¿ç”¨ RAG æ£€ç´¢è¡¨ç»“æ„ â†’ è°ƒç”¨ LLM ç”Ÿæˆ SQL
Author: KevinChen
"""

import os, json, argparse
from typing import List
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI, OpenAIError
import backoff

# ----------------- é»˜è®¤å‚æ•° -----------------
INDEX_PATH = "faiss_index.bin"
MAP_PATH = "table_mapping.json"
TABLE_JSON = "table.json"
SQL_TYPE = "mysql"
TOP_K = 10
MAX_LEN = 512
BATCH_SIZE = 16
LLM_MODEL = "deepseek-chat"  # å¯æ¢å…¶ä»–
API_KEY = "sk-xxxxx"
API_KEY = "sk-43a58ad2bbd740b095f5f61671ed9fae"
LLM_URL = "https://api.deepseek.com"
QUESTION = "å…¬å‹ŸåŸºé‡‘çš„äºŒçº§åˆ†ç±»åŸºé‡‘ç±»å‹æ˜¯è‚¡ç¥¨å‹çš„æœ€è¿‘1å¹´å‡€å€¼å’Œæ”¶ç›Šç‡æ•°æ®,åªè¦äº¤æ˜“æ—¥çš„æ•°æ®,ç”¨ä¸Šæµ·å¸‚åœºäº¤æ˜“æ—¥?"
DEFAULT_EMBED_MODEL_PATH = "/Users/chenjunming/.cache/huggingface/hub/models--BAAI--bge-m3/snapshots/5617a9f61b028005a4858fdac845db406aefb181"  # é»˜è®¤åµŒå…¥æ¨¡å‹è·¯å¾„

# --------- ç¦ç”¨å¹¶è¡Œ tokenizer çº¿ç¨‹ï¼Œé¿å…è„šæœ¬ä¸é€€å‡º ----------
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ---------------- LLM è°ƒç”¨ ----------------
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€åèµ„æ·±æ•°æ®å·¥ç¨‹å¸ˆï¼Œç²¾é€š SQL ç¼–å†™ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. **å€¼æ˜ å°„ï¼š** è‹¥å­—æ®µå¤‡æ³¨å·²ç»™ä¸­æ–‡â†”ä»£ç æ˜ å°„ï¼Œè¯·å…ˆæŠŠç”¨æˆ·æè¿°è½¬æ¢ä¸ºå¯¹åº”ä»£ç å†è¿‡æ»¤ã€‚
2. **è¡¨é€‰æ‹©ï¼š** åªé€‰æ‹©ä¸éœ€æ±‚ç›´æ¥ç›¸å…³çš„è¡¨ï¼Œé¿å…å†—ä½™ JOINã€‚
3. **JOIN æ¡ä»¶ï¼š** å½“ç¡®æœ‰å…³ç³»æ—¶ä½¿ç”¨è¡¨é—´å…³è”å­—æ®µã€‚
4. **æ³¨é‡Šï¼š** SQL åŠ ä¸Šä¸­æ–‡æ³¨é‡Šè§£é‡Šå­—æ®µå«ä¹‰ã€è¿‡æ»¤æ¡ä»¶åŠ JOIN é€»è¾‘ã€‚
"""


@backoff.on_exception(backoff.expo, OpenAIError, max_tries=3)
def call_llm(prompt: str,
             api_key: str,
             base_url: str,
             sql_type: str,
             model: str = LLM_MODEL,
             temperature: float = 0.1) -> str:
    client = OpenAI(api_key=api_key, base_url=base_url)
    sys_prompt = f"""
ä½ æ˜¯ä¸€åèµ„æ·±æ•°æ®å·¥ç¨‹å¸ˆï¼Œç²¾é€š{sql_type}æ•°æ®åº“çš„ SQL ç¼–å†™ã€‚è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. **å€¼æ˜ å°„ï¼š** è‹¥å­—æ®µå¤‡æ³¨å·²ç»™ä¸­æ–‡â†”ä»£ç æ˜ å°„ï¼Œè¯·å…ˆæŠŠç”¨æˆ·æè¿°è½¬æ¢ä¸ºå¯¹åº”ä»£ç å†è¿‡æ»¤ã€‚
2. **è¡¨é€‰æ‹©ï¼š** åªé€‰æ‹©ä¸éœ€æ±‚ç›´æ¥ç›¸å…³çš„è¡¨ï¼Œé¿å…å†—ä½™ JOINã€‚
3. **JOIN æ¡ä»¶ï¼š** å½“ç¡®æœ‰å…³ç³»æ—¶ä½¿ç”¨è¡¨é—´å…³è”å­—æ®µã€‚
4. **æ³¨é‡Šï¼š** SQL åŠ ä¸Šä¸­æ–‡æ³¨é‡Šè§£é‡Šå­—æ®µå«ä¹‰ã€è¿‡æ»¤æ¡ä»¶åŠ JOIN é€»è¾‘ã€‚
5. **æ‰§è¡Œæ•ˆç‡ï¼š** ä½ å†™çš„ SQL ä¸€å®šæ˜¯æ‰§è¡Œæ•ˆç‡æœ€é«˜çš„SQLä»£ç , ç»å¯¹ç¬¦åˆ{sql_type}æ•°æ®åº“çš„ç‰¹æ€§,è¯­æ³•,æ‰§è¡Œæ•ˆç‡ã€‚
    """
    resp = client.chat.completions.create(
        model=model,
        temperature=temperature,
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": prompt}
        ])
    return resp.choices[0].message.content.strip()


# ------------- æ„é€  prompt ----------------
def build_prompt(user_q: str, sql_type: str, ctx_blocks: List[str]) -> str:
    ctx_txt = "\n\n--- ç›¸å…³è¡¨ç»“æ„ ---\n" + "\n\n".join(ctx_blocks)
    return f"""{ctx_txt}

--- ç”¨æˆ·éœ€æ±‚ ---
{user_q}

è¯·åœ¨ ```sql ``` å—ä¸­ç»™å‡ºæœ€ç»ˆç¬¦åˆ{sql_type}è¯­æ³•çš„ SQL, è¦æœ‰è¯¦ç»†çš„å­—æ®µæ³¨é‡Š,å…³è”æ³¨é‡Š,ä»¥åŠæ¡ä»¶æ³¨é‡Šã€‚
å¹¶åœ¨ä»£ç å—ä¸­ç”¨æ³¨é‡Šè§£é‡Šæ‰€ç”¨è¡¨ã€å­—æ®µã€JOIN é€»è¾‘ã€‚å¯¹äºä½ ä¸çŸ¥é“çš„å­—æ®µå€¼æ˜ å°„å…³ç³»,ä½ è¦è¿›è¡Œè¯´æ˜ã€‚
è¯·å‹¿è¿”å›å…¶ä»–å†…å®¹ï¼Œåªè¿”å› ```sql ``` å—ã€‚è¯·æ³¨æ„SQLçš„æ‰§è¡Œæ•ˆç‡,å†™å‡ºæœ€ä½³æ€§èƒ½çš„sqlä»£ç ã€‚
    """


# å…¨å±€å˜é‡ç”¨äºç¼“å­˜åµŒå…¥æ¨¡å‹å®ä¾‹å’Œå½“å‰åŠ è½½çš„æ¨¡å‹è·¯å¾„
_embedder_instance = None
_current_embed_model_path = None


# ------------- ä¸»å‡½æ•° --------------------
def rag_sql(question: str,
            api_key: str,
            base_url: str,
            sql_type: str,
            embed_model_path: str,  # æ–°å¢å‚æ•°
            top_k: int = TOP_K):
    # 1. åŠ è½½èµ„æºï¼ˆå…¨å±€æ‡’åŠ è½½ï¼‰
    global faiss_index, id2text, _embedder_instance, _current_embed_model_path

    if "faiss_index" not in globals():
        faiss_index = faiss.read_index(INDEX_PATH)
        id2text = json.load(open(MAP_PATH, encoding="utf-8"))

    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½åµŒå…¥æ¨¡å‹
    if _embedder_instance is None or _current_embed_model_path != embed_model_path:
        yield f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {embed_model_path}..."
        _embedder_instance = SentenceTransformer(embed_model_path)
        _embedder_instance.max_seq_length = MAX_LEN
        _current_embed_model_path = embed_model_path
        yield "åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ."

    embedder = _embedder_instance  # ä½¿ç”¨ç¼“å­˜çš„å®ä¾‹

    # 2. ç¼–ç é—®é¢˜å‘é‡
    q_vec = embedder.encode([question], normalize_embeddings=True).astype("float32")

    # 3. æ£€ç´¢
    _, I = faiss_index.search(q_vec, top_k)
    ctx_blocks = [id2text[str(idx)] if isinstance(id2text, dict) else id2text[idx]
                  for idx in I[0]]
    yield f"ğŸ¯ Top-{top_k} å‘½ä¸­è¡¨ç´¢å¼•: {I[0]}"
    for i, block in enumerate(ctx_blocks):
        yield f"â€”â€” è¡¨ç»“æ„ Top-{i + 1} â€”â€”{block}"

    # 4. æ„é€  Prompt
    prompt = build_prompt(question, sql_type, ctx_blocks)
    yield f"ğŸ“¤ å‘é€ç»™å¤§æ¨¡å‹çš„Promptå¦‚ä¸‹ï¼š{prompt}"

    # 5. è°ƒç”¨ LLM
    final_answer = call_llm(prompt, api_key=api_key, base_url=base_url, sql_type=sql_type)
    yield "=== LLM å›å¤ ==="
    yield final_answer


# ---------------- CLI ----------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="RAG ç”Ÿæˆ SQL demo")
    parser.add_argument("--question", default=QUESTION, help="ç”¨æˆ·è‡ªç„¶è¯­è¨€é—®é¢˜")
    parser.add_argument("--k", type=int, default=TOP_K, help="æ£€ç´¢ top-k")
    parser.add_argument("--key", type=str, default=API_KEY, help="OpenAI API Key")
    parser.add_argument("--url", type=str, default=LLM_URL, help="OpenAI Base URL")
    parser.add_argument("--sql_type", type=str, default=SQL_TYPE, help="OpenAI Base URL")
    parser.add_argument("--embed_model_path", type=str, default=DEFAULT_EMBED_MODEL_PATH, help="åµŒå…¥æ¨¡å‹è·¯å¾„")
    args = parser.parse_args()

    # Iterate over the generator and print each yielded message
    for message in rag_sql(question=args.question, api_key=args.key, base_url=args.url, sql_type=args.sql_type,
                           embed_model_path=args.embed_model_path, top_k=args.k):
        print(message)
    os._exit(0)
